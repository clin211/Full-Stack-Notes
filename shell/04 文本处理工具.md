# Shell 文本处理工具

在 Shell 编程中，文本处理是一项核心技能。Unix/Linux 系统提供了强大的文本处理工具，如 grep、sed 和 awk，它们可以帮助你高效地处理和分析文本数据。本文档将详细介绍这些工具的基本用法。

## grep - 全局正则表达式搜索

`grep`（Global Regular Expression Print）是一个强大的文本搜索工具，用于在文件或标准输入中查找匹配指定模式的行。

### 基本语法

```bash
grep [选项] 模式 [文件...]
```

### 常用选项

- `-i`：忽略大小写
- `-v`：反向匹配，显示不包含匹配模式的行
- `-n`：显示匹配行的行号
- `-c`：只显示匹配行的数量
- `-l`：只显示包含匹配的文件名
- `-r` 或 `-R`：递归搜索目录
- `-w`：匹配整个单词而非一部分
- `-A n`：显示匹配行及其后的 n 行
- `-B n`：显示匹配行及其前的 n 行
- `-C n`：显示匹配行及其前后各 n 行
- `-E`：使用扩展正则表达式（相当于使用 egrep）
- `--color=auto`：高亮显示匹配的文本

### 基本示例

```bash
# 在文件中查找包含 "error" 的行
grep "error" log.txt

# 忽略大小写
grep -i "error" log.txt

# 显示行号
grep -n "error" log.txt

# 只显示不包含 "error" 的行
grep -v "error" log.txt

# 递归搜索当前目录下所有文件
grep -r "function" .

# 只显示完全匹配 "error" 单词的行
grep -w "error" log.txt

# 显示匹配行及其后 2 行
grep -A 2 "error" log.txt

# 显示包含 "error" 或 "warning" 的行
grep -E "error|warning" log.txt
```

### 使用正则表达式

`grep` 支持基本正则表达式（BRE）。若要使用扩展正则表达式（ERE），请使用 `grep -E` 或 `egrep`。

```bash
# 匹配以 "error" 开头的行
grep "^error" log.txt

# 匹配以 "done" 结尾的行
grep "done$" log.txt

# 匹配包含数字的行
grep "[0-9]" log.txt

# 匹配包含至少一个数字的行
grep -E "[0-9]+" log.txt

# 匹配包含 "error" 或 "warning" 的行
grep -E "error|warning" log.txt
```

### 常见用法

```bash
# 查找并计算匹配行数
grep -c "error" log.txt

# 在多个文件中搜索
grep "function" *.js

# 查找所有包含特定模式的文件
grep -l "TODO" *.py

# 在进程列表中查找特定进程
ps aux | grep "chrome"

# 检查命令是否存在
command -v fswatch >/dev/null || echo "命令不存在"
```

## sed - 流编辑器

`sed`（Stream Editor）是一个非交互式的文本流编辑器，可以对文本进行替换、删除、插入等操作。

### 基本语法

```bash
sed [选项] '命令' [文件...]
```

### 常用选项

- `-e 脚本`：添加命令脚本
- `-f 脚本文件`：添加脚本文件中的命令
- `-n`：不自动打印模式空间
- `-i`：直接编辑文件（原地替换）
- `-r` 或 `-E`：使用扩展正则表达式

### 基本命令

- `s/正则/替换/标志`：替换文本
- `d`：删除行
- `p`：打印行（通常与 -n 选项一起使用）
- `a\文本`：在匹配行后添加文本
- `i\文本`：在匹配行前添加文本
- `c\文本`：替换整行文本
- `y/源字符集/目标字符集/`：字符转换

### 替换命令标志

- `g`：全局替换（一行中所有匹配项）
- `数字`：仅替换第 n 个匹配项
- `p`：打印包含替换的行
- `w 文件`：将替换结果写入文件
- `i`：忽略大小写

### 基本示例

```bash
# 替换文本（第一次出现）
sed 's/old/new/' file.txt

# 全局替换（所有出现）
sed 's/old/new/g' file.txt

# 直接修改文件
sed -i 's/old/new/g' file.txt

# 删除空行
sed '/^$/d' file.txt

# 删除包含 "ERROR" 的行
sed '/ERROR/d' file.txt

# 只打印包含 "ERROR" 的行
sed -n '/ERROR/p' file.txt

# 只打印第 5-10 行
sed -n '5,10p' file.txt

# 在匹配行后添加文本
sed '/pattern/a\新文本' file.txt

# 将 "ERROR" 替换为红色高亮
sed 's/ERROR/\x1b[31m&\x1b[0m/g' file.txt
```

### 多命令执行

```bash
# 执行多个替换
sed -e 's/old/new/g' -e 's/foo/bar/g' file.txt

# 或者使用分号分隔
sed 's/old/new/g; s/foo/bar/g' file.txt
```

### 地址范围

```bash
# 仅对第 5 行操作
sed '5s/old/new/' file.txt

# 对第 5-10 行操作
sed '5,10s/old/new/' file.txt

# 从第 5 行到文件末尾
sed '5,$s/old/new/' file.txt

# 仅对包含 "pattern" 的行操作
sed '/pattern/s/old/new/' file.txt
```

### 高级用法

```bash
# 引用捕获组
sed 's/\(hello\) world/\1 universe/' file.txt

# 删除 HTML 标签
sed 's/<[^>]*>//g' file.html

# 转换路径格式 (Windows to Unix)
sed 's/\\/\//g' file.txt

# 格式化 JSON（简单缩进）
sed -E 's/\{/\{\n/g; s/,/,\n/g; s/\}/\n\}/g' file.json
```

## awk - 文本处理语言

`awk` 是一种强大的文本处理语言，专为处理数据流而设计，以列为导向。

### 基本语法

```bash
awk [选项] '模式 {动作}' [文件...]
```

### 常用选项

- `-F 分隔符`：指定字段分隔符
- `-v 变量=值`：设置变量
- `-f 脚本文件`：从文件中读取 awk 脚本

### 内置变量

- `$0`：当前整行内容
- `$1, $2, ...`：第 1, 2, ... 个字段
- `NF`：当前行的字段数
- `NR`：当前处理的行号
- `FS`：输入字段分隔符（默认为空格）
- `OFS`：输出字段分隔符
- `RS`：输入记录分隔符（默认为换行符）
- `ORS`：输出记录分隔符
- `FILENAME`：当前输入文件名

### 基本示例

```bash
# 打印文件的第 1 和第 3 列
awk '{print $1, $3}' file.txt

# 使用自定义分隔符（冒号）
awk -F: '{print $1, $3}' /etc/passwd

# 打印行号和行内容
awk '{print NR, $0}' file.txt

# 打印包含 "pattern" 的行
awk '/pattern/' file.txt

# 打印第 3 列大于 100 的行
awk '$3 > 100' file.txt

# 计算所有数字的总和
awk '{sum += $1} END {print sum}' numbers.txt

# 打印行中的最后一个字段
awk '{print $NF}' file.txt

# 打印超过 80 个字符的行
awk 'length($0) > 80' file.txt
```

### 条件语句

```bash
# if 条件
awk '{if ($3 > 100) print $1, $3}' file.txt

# if-else 条件
awk '{if ($3 > 100) print "大于 100:", $0; else print "小于等于 100:", $0}' file.txt
```

### 循环

```bash
# for 循环遍历所有字段
awk '{for (i=1; i<=NF; i++) print $i}' file.txt
```

### 内置函数

```bash
# 字符串长度
awk '{print length($1)}' file.txt

# 大小写转换
awk '{print toupper($1), tolower($2)}' file.txt

# 字符串替换
awk '{gsub(/old/, "new"); print}' file.txt

# 字符串分割
awk '{split($0, arr, ":"); print arr[1], arr[3]}' file.txt
```

### 统计和报告

```bash
# 计算每种类型的数量
awk '{count[$1]++} END {for (type in count) print type, count[type]}' types.txt

# 计算列的总和和平均值
awk '{sum+=$3; count++} END {print "总和:", sum, "平均值:", sum/count}' data.txt

# 打印文件的行数、单词数和字符数（类似 wc）
awk '{chars+=length($0)+1; words+=NF} END {print NR, words, chars}' file.txt
```

## 组合使用文本处理工具

这些工具的真正威力在于它们可以通过管道组合使用，形成强大的文本处理流水线。

### 示例 1：分析日志文件

```bash
# 提取错误信息并统计各类型出现次数
grep "ERROR" log.txt | awk '{print $4}' | sort | uniq -c | sort -nr
```

### 示例 2：处理 CSV 文件

```bash
# 提取 CSV 文件中特定列并进行计算
awk -F, '{if ($3 > 1000) sum+=$3} END {print "总金额:", sum}' sales.csv
```

### 示例 3：分析网站访问日志

```bash
# 统计访问最多的 IP 地址
awk '{print $1}' access.log | sort | uniq -c | sort -nr | head -10
```

### 示例 4：生成报告

```bash
# 生成简单的 HTML 表格
awk -F, 'BEGIN {print "<table>\n<tr><th>姓名</th><th>年龄</th></tr>"}
{print "<tr><td>" $1 "</td><td>" $2 "</td></tr>"}
END {print "</table>"}' data.csv > report.html
```

## 正则表达式基础

在文本处理中，正则表达式是一种强大的模式匹配工具。以下是一些基本的正则表达式元字符：

### 基本元字符

- `.`：匹配任意单个字符（除了换行符）
- `^`：匹配行的开始
- `$`：匹配行的结束
- `*`：匹配前面的字符零次或多次
- `+`：匹配前面的字符一次或多次（扩展正则表达式）
- `?`：匹配前面的字符零次或一次（扩展正则表达式）
- `[]`：字符类，匹配括号内的任一字符
- `[^]`：否定字符类，匹配不在括号内的任一字符
- `\`：转义字符，使特殊字符按原义解释
- `|`：或操作符（扩展正则表达式）
- `()`：分组（扩展正则表达式）
- `{n}`：匹配前面的字符恰好 n 次（扩展正则表达式）
- `{n,}`：匹配前面的字符至少 n 次（扩展正则表达式）
- `{n,m}`：匹配前面的字符 n 到 m 次（扩展正则表达式）

### 常用字符类

- `[0-9]`：匹配任何数字
- `[a-z]`：匹配任何小写字母
- `[A-Z]`：匹配任何大写字母
- `[a-zA-Z]`：匹配任何字母
- `[a-zA-Z0-9]`：匹配任何字母或数字

### 预定义字符类

- `\d`：数字，等价于 `[0-9]`
- `\D`：非数字，等价于 `[^0-9]`
- `\w`：单词字符，等价于 `[a-zA-Z0-9_]`
- `\W`：非单词字符
- `\s`：空白字符，等价于 `[ \t\n\r\f]`
- `\S`：非空白字符

### 正则表达式示例

```bash
# 匹配电子邮件地址（简化版）
grep -E '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' file.txt

# 匹配 IP 地址（简化版）
grep -E '([0-9]{1,3}\.){3}[0-9]{1,3}' file.txt

# 匹配日期格式 (YYYY-MM-DD)
grep -E '[0-9]{4}-[0-9]{2}-[0-9]{2}' file.txt

# 匹配 HTML 标签
grep -E '<[^>]+>' file.html
```

## 实际案例：分析 watch-chrome.sh 的输出

假设我们运行了之前的 `watch-chrome.sh` 脚本，它会生成关于 Chrome 文件变化的日志。现在，我们想分析这些日志，并提取有用的信息。

假设日志格式如下：

```
2023-05-20 15:30:45 [数据] /Users/username/Library/Application Support/Google/Chrome/Default/Cache/data_1: Removed
2023-05-20 15:30:46 [数据] /Users/username/Library/Application Support/Google/Chrome/Default/Cache/data_2: Created
2023-05-20 15:30:47 [缓存] /Users/username/Library/Caches/Google/Chrome/Default/Cache/index: Modified
```

我们可以使用文本处理工具来分析这些日志：

```bash
# 统计各种事件类型的数量
cat chrome_logs.txt | awk '{print $4}' | sort | uniq -c

# 查找所有被修改的文件
grep "Modified" chrome_logs.txt | awk '{print $3}'

# 统计每个目录的变化数量
cat chrome_logs.txt | awk -F'/' '{print $1"/"$2"/"$3"/"$4"/"$5"/"$6}' | sort | uniq -c

# 提取特定时间段的日志
sed -n '/2023-05-20 15:30/,/2023-05-20 15:35/p' chrome_logs.txt

# 统计每分钟的事件数量
awk '{print substr($1,1,16)}' chrome_logs.txt | sort | uniq -c
```

## 练习任务

1. 创建一个包含多行文本的文件，然后：
   - 使用 grep 提取包含特定单词的行
   - 使用 sed 替换所有出现的 "old" 为 "new"
   - 使用 awk 打印每行的第一个和最后一个字段

2. 解析以下格式的日志文件：

   ```
   2023-05-20 15:30:45 ERROR File not found: document.pdf
   2023-05-20 15:31:23 INFO  Processing started
   2023-05-20 15:32:10 WARN  Low disk space
   ```

   - 提取所有 ERROR 级别的日志
   - 统计每种日志级别的数量
   - 创建一个报告，显示每小时的日志数量

3. 处理以下格式的 CSV 文件：

   ```
   姓名,年龄,城市,收入
   张三,25,北京,8000
   李四,30,上海,12000
   王五,28,广州,9500
   ```

   - 计算平均年龄和总收入
   - 找出收入最高的人
   - 将数据格式化为 HTML 表格

## 小结

文本处理是 Shell 脚本中最常见和最有用的技能之一。通过掌握 grep、sed 和 awk 这三个强大的工具，你可以高效地处理各种文本数据，从简单的搜索替换到复杂的数据分析和报告生成。

这些工具的真正威力在于它们可以通过管道组合使用，创建强大的文本处理流水线。随着你的经验增长，你将能够构建越来越复杂和有用的文本处理解决方案。

## 进阶资源

- `man grep`, `man sed`, `man awk` - 详细的命令手册
- 《sed & awk》by Dale Dougherty and Arnold Robbins - 经典参考书
- 《The AWK Programming Language》by Alfred V. Aho, Brian W. Kernighan, and Peter J. Weinberger - awk 的权威指南
- <https://www.grymoire.com/Unix/Sed.html> - sed 详细教程
- <https://www.grymoire.com/Unix/Awk.html> - awk 详细教程
